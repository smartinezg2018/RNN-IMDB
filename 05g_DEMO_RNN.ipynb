{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part g: RNN DEMO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs to classify sentiment on IMDB data\n",
    "For this exercise, we will train a \"vanilla\" RNN to predict the sentiment on IMDB reviews.  Our data consists of 25000 training sequences and 25000 test sequences.  The outcome is binary (positive/negative) and both outcomes are equally represented in both the training and the test set.\n",
    "\n",
    "Keras provides a convenient interface to load the data and immediately encode the words into integers (based on the most common words).  This will save us a lot of the drudgery that is usually involved when working with raw text.\n",
    "\n",
    "We will walk through the preparation of the data and the building of an RNN model.  Then it will be your turn to build your own models (and prepare the data how you see fit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 20:35:32.706436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-22 20:35:32.858090: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-22 20:35:32.867898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-01-22 20:35:32.867948: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-01-22 20:35:33.747587: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-22 20:35:33.747701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-22 20:35:33.747715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 30  # maximum length of a sequence - truncate after this\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "## Load in the data.  The function automatically tokenizes the text into distinct integers\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 30)\n",
      "x_test shape: (25000, 30)\n"
     ]
    }
   ],
   "source": [
    "# This pads (or truncates) the sequences so that they are of the maximum length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  219,   141,    35,   221,   956,    54,    13,    16,    11,\n",
       "        2714,    61,   322,   423,    12,    38,    76,    59,  1803,\n",
       "          72,     8, 10508,    23,     5,   967,    12,    38,    85,\n",
       "          62,   358,    99], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[123,:]  #Here's what an example sequence looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras layers for (Vanilla) RNNs\n",
    "\n",
    "In this exercise, we will not use pre-trained word vectors.  Rather we will learn an embedding as part of the Neural Network.  This is represented by the Embedding Layer below.\n",
    "\n",
    "### Embedding Layer\n",
    "`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n",
    "\n",
    "- This layer maps each integer into a distinct (dense) word vector of length `output_dim`.\n",
    "- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe)\n",
    "- The `input_dim` should be the size of the vocabulary.\n",
    "- The `input_length` specifies the length of the sequences that the network expects.\n",
    "\n",
    "### SimpleRNN Layer\n",
    "`keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- This is the basic RNN, where the output is also fed back as the \"hidden state\" to the next iteration.\n",
    "- The parameter `units` gives the dimensionality of the output (and therefore the hidden state).  Note that typically there will be another layer after the RNN mapping the (RNN) output to the network output.  So we should think of this value as the desired dimensionality of the hidden state and not necessarily the desired output of the network.\n",
    "- Recall that there are two sets of weights, one for the \"recurrent\" phase and the other for the \"kernel\" phase.  These can be configured separately in terms of their initialization, regularization, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 20:35:40.321465: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-01-22 20:35:40.321533: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-01-22 20:35:40.321647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyterlab-simonmartine): /proc/driver/nvidia/version does not exist\n",
      "2025-01-22 20:35:40.322176: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## Let's build a RNN\n",
    "\n",
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 50\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence and embeds it in a 50-dimensional vector\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1000000   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 5)                 280       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,000,286\n",
      "Trainable params: 1,000,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Note that most of the parameters come from the embedding layer\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 43s 54ms/step - loss: 0.6869 - accuracy: 0.5666 - val_loss: 0.6679 - val_accuracy: 0.6136\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.6313 - accuracy: 0.6547 - val_loss: 0.6051 - val_accuracy: 0.6721\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.5690 - accuracy: 0.7050 - val_loss: 0.5660 - val_accuracy: 0.7031\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.5238 - accuracy: 0.7395 - val_loss: 0.5441 - val_accuracy: 0.7193\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.4890 - accuracy: 0.7660 - val_loss: 0.5213 - val_accuracy: 0.7345\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.4609 - accuracy: 0.7842 - val_loss: 0.5024 - val_accuracy: 0.7489\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.4375 - accuracy: 0.8001 - val_loss: 0.4938 - val_accuracy: 0.7541\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.4193 - accuracy: 0.8088 - val_loss: 0.4814 - val_accuracy: 0.7634\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.4041 - accuracy: 0.8197 - val_loss: 0.4717 - val_accuracy: 0.7708\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.3914 - accuracy: 0.8260 - val_loss: 0.4682 - val_accuracy: 0.7741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f706107bd90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 11ms/step - loss: 0.4682 - accuracy: 0.7741\n",
      "Test score: 0.4682003855705261\n",
      "Test accuracy: 0.7741199731826782\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_rnn.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise, we will illustrate:\n",
    "- Preparing the data to use sequences of length 80 rather than length 30.  Does it improve the performance?\n",
    "- Trying different values of the \"max_features\".  Does this  improve the performance?\n",
    "- Trying smaller and larger sizes of the RNN hidden dimension.  How does it affect the model performance?  How does it affect the run time?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 80  # maximum length of a sequence - truncate after this\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 50\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 83s 105ms/step - loss: 0.6223 - accuracy: 0.6488 - val_loss: 0.5614 - val_accuracy: 0.7034\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 81s 103ms/step - loss: 0.4693 - accuracy: 0.7756 - val_loss: 0.4615 - val_accuracy: 0.7758\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 89s 114ms/step - loss: 0.3968 - accuracy: 0.8232 - val_loss: 0.4345 - val_accuracy: 0.8014\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 123s 158ms/step - loss: 0.3521 - accuracy: 0.8471 - val_loss: 0.3957 - val_accuracy: 0.8219\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 115s 147ms/step - loss: 0.3226 - accuracy: 0.8645 - val_loss: 0.3861 - val_accuracy: 0.8266\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 113s 144ms/step - loss: 0.3011 - accuracy: 0.8745 - val_loss: 0.3765 - val_accuracy: 0.8320\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 0.2851 - accuracy: 0.8835 - val_loss: 0.3771 - val_accuracy: 0.8335\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 117s 150ms/step - loss: 0.2721 - accuracy: 0.8898 - val_loss: 0.3701 - val_accuracy: 0.8360\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2608 - accuracy: 0.8958 - val_loss: 0.3736 - val_accuracy: 0.8368\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.2512 - accuracy: 0.8980 - val_loss: 0.3652 - val_accuracy: 0.8421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f705a50d450>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features = 5000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 80  # maximum length of a sequence - truncate after this\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 20\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 100s 125ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 94s 120ms/step - loss: 0.6850 - accuracy: 0.5197 - val_loss: 0.6626 - val_accuracy: 0.6129\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.5828 - accuracy: 0.7060 - val_loss: 0.5565 - val_accuracy: 0.7180\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.4841 - accuracy: 0.7720 - val_loss: 0.4831 - val_accuracy: 0.7765\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.4337 - accuracy: 0.8048 - val_loss: 0.4392 - val_accuracy: 0.7946\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4028 - accuracy: 0.8207 - val_loss: 0.4252 - val_accuracy: 0.8073\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.3807 - accuracy: 0.8340 - val_loss: 0.4256 - val_accuracy: 0.8082\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3641 - accuracy: 0.8444 - val_loss: 0.4102 - val_accuracy: 0.8091\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 66s 84ms/step - loss: 0.3510 - accuracy: 0.8493 - val_loss: 0.3941 - val_accuracy: 0.8220\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3403 - accuracy: 0.8545 - val_loss: 0.4112 - val_accuracy: 0.8158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7059106a10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3311 - accuracy: 0.8592 - val_loss: 0.3961 - val_accuracy: 0.8150\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.3233 - accuracy: 0.8628 - val_loss: 0.3788 - val_accuracy: 0.8286\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.3163 - accuracy: 0.8652 - val_loss: 0.3752 - val_accuracy: 0.8306\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 66s 85ms/step - loss: 0.3099 - accuracy: 0.8692 - val_loss: 0.3759 - val_accuracy: 0.8300\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.3033 - accuracy: 0.8727 - val_loss: 0.3734 - val_accuracy: 0.8324\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 79s 102ms/step - loss: 0.2990 - accuracy: 0.8757 - val_loss: 0.3938 - val_accuracy: 0.8217\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2934 - accuracy: 0.8773 - val_loss: 0.3638 - val_accuracy: 0.8375\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2889 - accuracy: 0.8799 - val_loss: 0.3602 - val_accuracy: 0.8385\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 66s 84ms/step - loss: 0.2843 - accuracy: 0.8819 - val_loss: 0.3691 - val_accuracy: 0.8368\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2811 - accuracy: 0.8820 - val_loss: 0.3912 - val_accuracy: 0.8274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f705e0cfb90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out of curiosity, run for 10 more epochs\n",
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
